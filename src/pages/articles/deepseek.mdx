
import { ArticleLayout } from '@/components/ArticleLayout'

export const meta = {
  author: 'Yang',
  date: '2025-02-14',
  title: '手把手教你部署DeepSeek，并添加自己的知识库和自定义模型',
  description:
    'Ubuntu，NVD, DeepSeek, 知识库，自定义模型',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

春节前，中国的DeepSeek大火，它的训练成本比OpenAI小了接近100倍，让美股大跌，我在支付宝买的美股纳斯达克综合指数基金也跌了3个点，英伟达的股票直接跌了18个点，这个大模型
真是太厉害，如此厉害，我一定要尝试一下，春节前就使用上了，体验了一下，效果的确很不错，和ChatGPT很接近，并且它还是免费的。

正式上班后，再去使用，发现DeepSeek官网总是崩溃，于是着手用公司的机器自己部署，下面是我的部署步骤，尽量选择科学上哇那个。


### 安装Ollama

1. 先介绍一下我的安装环境
  ```
  systemOS       Ubuntu 24.04.1 LTS
  system         Precision 3660 (0A9F)
  processor      12th Gen Intel(R) Core(TM) i9-12900K
  memory         32GiB System Memory
  storage        Samsung SSD 980 PRO 1TB
  GPU            NVIDIA GeForce RTX 3060 Ti 8192MiB

  ollama 可能需要自备🪜
  ```

2. 在Ubuntu下载Ollama

  `curl -fsSL https://ollama.com/install.sh | sh`

  运行 ollama `ollama serve`, 一路输入yes

  
3. 通过ollama下载DeepSeek模型

  `ollama run deepseek-r1:7b`

  这一步安装成功后，就可以在terminal对话了

4. ollama API 端口暴露出来（💡 Ollama 的 API 接口默认只监听 localhost（127.0.0.1）接口，而不是所有外部网络地址。）

  ```
     # 在本机测试可以：

     curl http://localhost:11434/api/generate \
      -d '{
        "model": "deepseek-r1:14b",
        "prompt": "用一句话介绍中国的万里长城。",
        "stream": false
      }'

    # 但是另外一台机器，不能用，可以ping通
      curl http://192.168.0.107:11434/api/generate \
      -d '{
        "model": "deepseek-r1:14b",
        "prompt": "用一句话介绍中国的万里长城。",
        "stream": false
      }'

    # 解决办法
 
    在 ~/.bashrc 文件后面 加上
    export OLLAMA_HOST=0.0.0.0

    保存后 
    source ~/.bashrc 

    然后重新运行
    ollama serve
  ```

5. 创建 systemd 服务：让 Ollama 开机自启

  vim /etc/systemd/system/ollama.service

  ```
  [Unit]
  Description=Ollama Service
  After=network-online.target

  [Service]
  ExecStart=/usr/local/bin/ollama serve
  User=ollama
  Group=ollama
  Restart=always
  RestartSec=3
  Environment=OLLAMA_HOST=0.0.0.0
  Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin"

  [Install]
  WantedBy=default.target
  ```

  重启和验证

  ```
  sudo systemctl daemon-reload
  sudo systemctl restart ollama

  systemctl status ollama

  ```
### 安装 Open WebUI

1. 安装 Open WebUI

  这里官网提供用Python和Docker两种安装方法，用Python方法我尝试了两次没有成功，比较麻烦，于是我选择使用docker安装

  ```
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

  这里我的显卡是NVD的，如果不是可以去掉 `--gpus all` 参数,

  `-e OLLAMA_BASE_URL=https://example.com`  参数可以修改ollama的地址

  这里我遇到一个问题，Open WebUI已经启动了，通过 `localhost:3000` 也能访问到GUI，但是没有获取docker容器外，Ubuntu宿主机安装的Deepseek模型，这里官方提供了一种方法，可以试试

  ```
  Open WebUI: Server Connection Error

  If you're experiencing connection issues, it’s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . 
  Use the --network=host flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: http://localhost:8080.
  ```

  但是这样记得GUI端口就改成 `8080`了


  另外我自己的解决docker不能访问11434端口的解决办法

  ```
    sudo vim /etc/systemd/system/ollama.service
    查看环境变量位置
    EnvironmentFile=/etc/environment

    查看 ollama serve -h

    OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)

    这里可以确认 ollama 只监听本地的11434，docker内的环境是隔离的，不能访问11434

    sudo nano /etc/environment

    把 OLLAMA_HOST=0.0.0.0:11434 加到环境变量最后

    重启 ollama
    
  ```

2. 部署结果，成功了，就可以愉快聊天了

![openwebui](/images/tech/openwebui.png)

3. 喂大模型自己的数据

   1. 添加Knowledge

   ![add_knowledge](/images/tech/add_knowledge.png)

   2. 测试Knowledge

   knowledge 添加后，在对话页面，先输入#键，就可以引用到你的知识库，（这个功能在2024年 GPT 4 Plus功能 花费每个月20美元才能使用的）

   ![test_knowledge](/images/tech/test_knowledge.png)
   
   3. 添加Model 

   ![add_model](/images/tech/add_model.png)

   4. 测试Model

   ![test_model](/images/tech/test_model.png)

