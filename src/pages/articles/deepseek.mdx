
import { ArticleLayout } from '@/components/ArticleLayout'

export const meta = {
  author: 'Yang',
  date: '2025-02-14',
  title: 'æ‰‹æŠŠæ‰‹æ•™ä½ éƒ¨ç½²DeepSeekï¼Œå¹¶æ·»åŠ è‡ªå·±çš„çŸ¥è¯†åº“å’Œè‡ªå®šä¹‰æ¨¡å‹',
  description:
    'Ubuntuï¼ŒNVD, DeepSeek, çŸ¥è¯†åº“ï¼Œè‡ªå®šä¹‰æ¨¡å‹',
}

export default (props) => <ArticleLayout meta={meta} {...props} />

æ˜¥èŠ‚å‰ï¼Œä¸­å›½çš„DeepSeekå¤§ç«ï¼Œå®ƒçš„è®­ç»ƒæˆæœ¬æ¯”OpenAIå°äº†æ¥è¿‘100å€ï¼Œè®©ç¾è‚¡å¤§è·Œï¼Œæˆ‘åœ¨æ”¯ä»˜å®ä¹°çš„ç¾è‚¡çº³æ–¯è¾¾å…‹ç»¼åˆæŒ‡æ•°åŸºé‡‘ä¹Ÿè·Œäº†3ä¸ªç‚¹ï¼Œè‹±ä¼Ÿè¾¾çš„è‚¡ç¥¨ç›´æ¥è·Œäº†18ä¸ªç‚¹ï¼Œè¿™ä¸ªå¤§æ¨¡å‹
çœŸæ˜¯å¤ªå‰å®³ï¼Œå¦‚æ­¤å‰å®³ï¼Œæˆ‘ä¸€å®šè¦å°è¯•ä¸€ä¸‹ï¼Œæ˜¥èŠ‚å‰å°±ä½¿ç”¨ä¸Šäº†ï¼Œä½“éªŒäº†ä¸€ä¸‹ï¼Œæ•ˆæœçš„ç¡®å¾ˆä¸é”™ï¼Œå’ŒChatGPTå¾ˆæ¥è¿‘ï¼Œå¹¶ä¸”å®ƒè¿˜æ˜¯å…è´¹çš„ã€‚

æ­£å¼ä¸Šç­åï¼Œå†å»ä½¿ç”¨ï¼Œå‘ç°DeepSeekå®˜ç½‘æ€»æ˜¯å´©æºƒï¼Œäºæ˜¯ç€æ‰‹ç”¨å…¬å¸çš„æœºå™¨è‡ªå·±éƒ¨ç½²ï¼Œä¸‹é¢æ˜¯æˆ‘çš„éƒ¨ç½²æ­¥éª¤ï¼Œå°½é‡é€‰æ‹©ç§‘å­¦ä¸Šå“‡é‚£ä¸ªã€‚


### å®‰è£…Ollama

1. å…ˆä»‹ç»ä¸€ä¸‹æˆ‘çš„å®‰è£…ç¯å¢ƒ
  ```
  systemOS       Ubuntu 24.04.1 LTS
  system         Precision 3660 (0A9F)
  processor      12th Gen Intel(R) Core(TM) i9-12900K
  memory         32GiB System Memory
  storage        Samsung SSD 980 PRO 1TB
  GPU            NVIDIA GeForce RTX 3060 Ti 8192MiB

  ollama å¯èƒ½éœ€è¦è‡ªå¤‡ğŸªœ
  ```

2. åœ¨Ubuntuä¸‹è½½Ollama

  `curl -fsSL https://ollama.com/install.sh | sh`

  è¿è¡Œ ollama `ollama serve`, ä¸€è·¯è¾“å…¥yes

  
3. é€šè¿‡ollamaä¸‹è½½DeepSeekæ¨¡å‹

  `ollama run deepseek-r1:7b`

  è¿™ä¸€æ­¥å®‰è£…æˆåŠŸåï¼Œå°±å¯ä»¥åœ¨terminalå¯¹è¯äº†

4. ollama API ç«¯å£æš´éœ²å‡ºæ¥ï¼ˆğŸ’¡ Ollama çš„ API æ¥å£é»˜è®¤åªç›‘å¬ localhostï¼ˆ127.0.0.1ï¼‰æ¥å£ï¼Œè€Œä¸æ˜¯æ‰€æœ‰å¤–éƒ¨ç½‘ç»œåœ°å€ã€‚ï¼‰

  ```
     # åœ¨æœ¬æœºæµ‹è¯•å¯ä»¥ï¼š

     curl http://localhost:11434/api/generate \
      -d '{
        "model": "deepseek-r1:14b",
        "prompt": "ç”¨ä¸€å¥è¯ä»‹ç»ä¸­å›½çš„ä¸‡é‡Œé•¿åŸã€‚",
        "stream": false
      }'

    # ä½†æ˜¯å¦å¤–ä¸€å°æœºå™¨ï¼Œä¸èƒ½ç”¨ï¼Œå¯ä»¥pingé€š
      curl http://192.168.0.107:11434/api/generate \
      -d '{
        "model": "deepseek-r1:14b",
        "prompt": "ç”¨ä¸€å¥è¯ä»‹ç»ä¸­å›½çš„ä¸‡é‡Œé•¿åŸã€‚",
        "stream": false
      }'

    # è§£å†³åŠæ³•
 
    åœ¨ ~/.bashrc æ–‡ä»¶åé¢ åŠ ä¸Š
    export OLLAMA_HOST=0.0.0.0

    ä¿å­˜å 
    source ~/.bashrc 

    ç„¶åé‡æ–°è¿è¡Œ
    ollama serve
  ```

5. åˆ›å»º systemd æœåŠ¡ï¼šè®© Ollama å¼€æœºè‡ªå¯

  vim /etc/systemd/system/ollama.service

  ```
  [Unit]
  Description=Ollama Service
  After=network-online.target

  [Service]
  ExecStart=/usr/local/bin/ollama serve
  User=ollama
  Group=ollama
  Restart=always
  RestartSec=3
  Environment=OLLAMA_HOST=0.0.0.0
  Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin"

  [Install]
  WantedBy=default.target
  ```

  é‡å¯å’ŒéªŒè¯

  ```
  sudo systemctl daemon-reload
  sudo systemctl restart ollama

  systemctl status ollama

  ```
### å®‰è£… Open WebUI

1. å®‰è£… Open WebUI

  è¿™é‡Œå®˜ç½‘æä¾›ç”¨Pythonå’ŒDockerä¸¤ç§å®‰è£…æ–¹æ³•ï¼Œç”¨Pythonæ–¹æ³•æˆ‘å°è¯•äº†ä¸¤æ¬¡æ²¡æœ‰æˆåŠŸï¼Œæ¯”è¾ƒéº»çƒ¦ï¼Œäºæ˜¯æˆ‘é€‰æ‹©ä½¿ç”¨dockerå®‰è£…

  ```
  docker run -d -p 3000:8080 --gpus all --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:cuda
  ```

  è¿™é‡Œæˆ‘çš„æ˜¾å¡æ˜¯NVDçš„ï¼Œå¦‚æœä¸æ˜¯å¯ä»¥å»æ‰ `--gpus all` å‚æ•°,

  `-e OLLAMA_BASE_URL=https://example.com`  å‚æ•°å¯ä»¥ä¿®æ”¹ollamaçš„åœ°å€

  è¿™é‡Œæˆ‘é‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼ŒOpen WebUIå·²ç»å¯åŠ¨äº†ï¼Œé€šè¿‡ `localhost:3000` ä¹Ÿèƒ½è®¿é—®åˆ°GUIï¼Œä½†æ˜¯æ²¡æœ‰è·å–dockerå®¹å™¨å¤–ï¼ŒUbuntuå®¿ä¸»æœºå®‰è£…çš„Deepseekæ¨¡å‹ï¼Œè¿™é‡Œå®˜æ–¹æä¾›äº†ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥è¯•è¯•

  ```
  Open WebUI: Server Connection Error

  If you're experiencing connection issues, itâ€™s often due to the WebUI docker container not being able to reach the Ollama server at 127.0.0.1:11434 (host.docker.internal:11434) inside the container . 
  Use the --network=host flag in your docker command to resolve this. Note that the port changes from 3000 to 8080, resulting in the link: http://localhost:8080.
  ```

  ä½†æ˜¯è¿™æ ·è®°å¾—GUIç«¯å£å°±æ”¹æˆ `8080`äº†


  å¦å¤–æˆ‘è‡ªå·±çš„è§£å†³dockerä¸èƒ½è®¿é—®11434ç«¯å£çš„è§£å†³åŠæ³•

  ```
    sudo vim /etc/systemd/system/ollama.service
    æŸ¥çœ‹ç¯å¢ƒå˜é‡ä½ç½®
    EnvironmentFile=/etc/environment

    æŸ¥çœ‹ ollama serve -h

    OLLAMA_HOST                IP Address for the ollama server (default 127.0.0.1:11434)

    è¿™é‡Œå¯ä»¥ç¡®è®¤ ollama åªç›‘å¬æœ¬åœ°çš„11434ï¼Œdockerå†…çš„ç¯å¢ƒæ˜¯éš”ç¦»çš„ï¼Œä¸èƒ½è®¿é—®11434

    sudo nano /etc/environment

    æŠŠ OLLAMA_HOST=0.0.0.0:11434 åŠ åˆ°ç¯å¢ƒå˜é‡æœ€å

    é‡å¯ ollama
    
  ```

2. éƒ¨ç½²ç»“æœï¼ŒæˆåŠŸäº†ï¼Œå°±å¯ä»¥æ„‰å¿«èŠå¤©äº†

![openwebui](/images/tech/openwebui.png)

3. å–‚å¤§æ¨¡å‹è‡ªå·±çš„æ•°æ®

   1. æ·»åŠ Knowledge

   ![add_knowledge](/images/tech/add_knowledge.png)

   2. æµ‹è¯•Knowledge

   knowledge æ·»åŠ åï¼Œåœ¨å¯¹è¯é¡µé¢ï¼Œå…ˆè¾“å…¥#é”®ï¼Œå°±å¯ä»¥å¼•ç”¨åˆ°ä½ çš„çŸ¥è¯†åº“ï¼Œï¼ˆè¿™ä¸ªåŠŸèƒ½åœ¨2024å¹´ GPT 4 PlusåŠŸèƒ½ èŠ±è´¹æ¯ä¸ªæœˆ20ç¾å…ƒæ‰èƒ½ä½¿ç”¨çš„ï¼‰

   ![test_knowledge](/images/tech/test_knowledge.png)
   
   3. æ·»åŠ Model 

   ![add_model](/images/tech/add_model.png)

   4. æµ‹è¯•Model

   ![test_model](/images/tech/test_model.png)

